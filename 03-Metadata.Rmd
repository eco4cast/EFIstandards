# FORECAST DATASET METADATA

## Summary and Design Assumptions

Although the EFI output file convention provides data format metadata, it does not by itself provide sufficient metadata on the forecast dataset itself to be able to understand how a forecast was generated or what assumptions and uncertainties are included in the forecast. Therefore, EFI has also developed a forecast dataset metadata convention (referred to as the “EFI metadata convention” below) to help set community expectations about what information needs to be archived about forecasts and to do so in a standard, interoperable format. In developing the EFI metadata convention, we tried to balance two competing demands: usability versus synthesis. 

On the usability side, the EFI metadata convention was developed with a focus on simplicity and usability. In an ideal world, it would be useful to have a lot of detailed information about a forecast, the underlying model used to make the forecast, and the workflow the forecast model is embedded in. However, such a convention would not be used in practice if this required a lot of additional work. The EFI convention aims to balance the metadata needs specific to forecasting against the practical aim of producing a standard that forecast producers will adhere to and forecast users will reference. Because a metadata format already in wide use by the ecological community is desirable for its utility and familiarity, we selected the Ecological Metadata Language (EML; https://eml.ecoinformatics.org/) as our base (Fegraus et al. 2005). EML is an XML-based metadata standard that has a long development history in ecology and is interconvertible with many other standards. EML also has the built-in extensibility, using the additionalMetadata space within the EML schema (https://eml.ecoinformatics.org/schema), that allows us to add forecast-specific information while continuing to produce valid and interoperable EML. That said, like with the output standard, the EFI metadata convention is potentially extensible to other metadata standards (e.g., ISO 19115, SpatioTemporal Asset Catalogue [STAC]).

On the synthesis side, a key goal of the EFI metadata convention was to address the needs of users working with multiple forecasts for different systems, and in particular to support those working on across-forecast syntheses and analyzes. In discussions with EFI’s Theory working group, key needs that emerged were: (1) the importance of recording the different sources of uncertainty that were considered in a forecast and how they were propagated; (2) a way of having simple proxies for the complexity of a model (e.g., number of parameters, number of covariates/drivers), and (3) a need to set some base EML variables as required for a forecast that might otherwise be optional. The specifics of how to use base EML to document a forecast, and which variables are required, are provided in Appendix S2. 

In many ways the metadata about forecast outputs shares many of the same characteristics as any other dataset, where documentation is needed for information like file format, variables, spatial and temporal resolution and extent, and provenance. However, forecast outputs have additional characteristics that separate them from observational data, as well as a few features that separate forecasts from most model outputs (e.g., for forecasts that are made repeatedly, it is not uncommon to make multiple different predictions for the same day that vary in the day the forecast was issued). To store this forecast-specific metadata we leverage the extensibility of the EML standard using the “additionalMetadata” field (Box 2). Many of the added elements are conceptually straightforward and provide information about forecast time step, global attributes (Table 1), and modeling approaches (see Appendix S3 for definitions of these EML elements). That said, one of the most important and novel contributions of the EFI metadata convention is a formalization of how we describe and account for the different uncertainties that are included in any particular forecast and how they relate to model structure, which is described in the following section.

```
<?xml version="1.0" encoding="UTF-8"?>
<eml:eml>
  <dataset>
      <title>
      <pubDate>
      <intellectualRights>
       ….
  </dataset>
  <additionalMetadata>
      ….
  </additionalMetadata>
</eml:eml>
```
Figure 4: Example high-level structure of an EML file.


## Forecast Uncertainty
Knowing how a forecasting approach handles different uncertainties is a critical part of its high-level structure, and is important to be able to interpret a forecast and fairly compare among different forecasts. For example, if a forecast that considers more uncertainties has a wider predictive interval, that does not necessarily mean it is doing “worse” than a model that considers fewer uncertainties. Indeed, forecasts that consider fewer uncertainties are more likely to be (falsely) overconfident.

Following the classification presented by (Dietze 2017b, 2017a), we assume the following general forecasting model, f

Eqn. 1
$$
Z_t ∼ g(Y_t|\phi)					
$$

$$
Yt=f(Y_{t-1,}X_{t} | \theta + \alpha_t) + \epsilon_t
$$ 


Where:
Y is the vector of the unobserved “true” latent state of the variables being predicted,
Z are observed/observable values of the variables of interest,
g is a probability distribution with parameters  that accounts for observation errors on Y and observation processes, including “observation operators” (i.e., any transformation between the observed state and the latent state), 
X are any drivers, covariates, or exogenous scenarios,
$\theta$ are the model’s parameters,
$\alpha$ describes the unexplained variability in model parameters (e.g., random effects),
$\epsilon$ is the process error, and
t is the dimension being forecasted along (typically time, but could also be space, phylogenetic distance, community similarity distance, network distance, etc.).

This framework is based on the long-established and frequently used structure of Hidden Markov models (a.k.a. state space models), which often include all of the terms described above, as well as that of iterative data assimilation algorithms (e.g., Kalman Filters, Particle Filters, variational data assimilation), which are widely used in ecological forecasting and represent special cases of Hidden Markov models [@wikle_bayesian_2007; @auger-methe_guide_2021]. That said, for any particular forecast any of the above terms may be absent. For example, in a simple linear model the function f does not include $Y_{t-1$, $\alpha_t$, or $\epsilon_t$, leaving just $Y_t=f(X_t | \theta)$, and all residual error is assumed to be Gaussian observation error, $g(Y_t | \phi)=N(Y_t,\sigma^2)$. Generalized linear models and a wide range of machine learning algorithms have essentially the same high-level structure as linear models but a more flexible choice of observation error / cost function (and in the case of machine learning, a more flexible representation of f), whereas generalized linear mixed models and generalized additive models are the same but add back in random effects, t. Classic timeseries forecasts (e.g., autoregressive integrated moving average [ARIMA] models) and recurrent neural networks (RNN) would include previous Y’s but typically not $X_t$, $\alpha_t$, or $\epsilon_t$ giving $Y_t=f(Y_{t-1} | \theta )$. Note that the framework above easily generalizes to continuous-time forecasts, but does assume that model outputs are stored at specific discrete times.

Given this framework, there are six REQUIRED elements that are used to provide basic information about model structure and how the forecast handles different uncertainties, although in any particular application this element may simply be used to indicate that a specific term is absent from that model (Table 6).

```{r,echo=FALSE}
knitr::kable(
tibble::tribble(
~Tag,                ~Description,
"&lt;initial_conditions&gt;", "**Uncertainty in the initialization of state variables (Y).** Initial condition uncertainty will be a common feature of any dynamic model, where the future state depends on the current state, such as population models, process-based biogeochemical pool & flux models, and classic time-series analysis. For time series models with multiple lags or dynamic models with memory, the initial conditions may cover multiple timepoints. Initial condition uncertainty will be absent from many statistical and machine learning models. Initial condition uncertainty might be directly informed by field data, indirectly inferred from other proxies (e.g., remote sensing), sampled from some (informed or uninformed) prior distribution, or “spun up” through model simulation. When spun up, initial condition uncertainty may have strong interactions with the other uncertainties below.",
"&lt;drivers&gt;",           "**Uncertainty in model drivers, covariates, and exogenous scenarios (X).** Driver/covariate uncertainties may come directly from a data product, as a reported error estimate or through driver ensembles, or may be estimated based on sampling theory, calibration/validation documents, or some other source. In most of these cases these uncertainties are thought about probabilistically. When making projections, driver uncertainty may also be associated with scenarios or decision alternatives. These alternative drivers are not themselves probabilistic (they do not have weights or probabilities) and forecast outputs are conditional on a specific alternative scenario. Examples include climate scenarios or treatments associated with system inputs (irrigation, fertilization, etc).",
"&lt;parameters&gt;",       "**Uncertainty in model parameters (&theta;).** For most ecological processes the parameters (a.k.a. coefficients) in model equations are not physical constants but need to be estimated from data. Because parameters are estimated from data, uncertainty will be associated with them. Parameter uncertainty is usually conditional on model structure and may be estimated directly from data (e.g., ecological traits) or indirectly (e.g., optimization or Bayesian calibration) by comparing model outputs to observations. Parameter uncertainty tends to decline asymptotically with sample size.",
"&lt;random_effects&gt;",    "**Unexplained variability and heterogeneity in model parameters (&alpha;).** Hierarchical models, random effect models, and meta transfer learning approaches all attempt to acknowledge that the ‘best’ model parameters may change across space, time, individual, or other measurement unit. This variability can be estimated and partitioned into different sources but is (as of yet) not explained within the model’s internal structure. Unlike parameter uncertainty, this variability in parameters does not decline with sample size. Example: variability/heterogeneity in ecological traits such as carbon-to-nitrogen ratios.",
"&lt;obs_error&gt;",          "**Uncertainty in the observations of the output variables (g).** Note that many statistical modeling approaches do not formally partition errors in observations from errors in the modeling process, but simply lump these into a residual error. We make the pragmatic distinction that errors that do not directly propagate into the future be recorded as observation errors. Observation errors now may indeed affect the initial condition uncertainty in the next forecast, but we consider this to be indirect.",
"&lt;process_error&gt;",       "**Dynamic uncertainty in the process model (&epsilon;)** attributable to both model misspecification (a.k.a. structural error) and stochasticity. Pragmatically, this is the portion of the residual error from one timestep to the next that is not attributable to any of the other uncertainties listed above, and which typically propagates into the future. Philosophically, process error (as defined here) convolves uncertainty that is part of the natural process itself (i.e., stochasticity), human ignorance about the true process (e.g., model structure), and errors associated with numerical approximation. Deconvolving these is both pragmatically and philosophically very challenging, but teams wishing to do so can alternatively use the &lt;stochastic_error&gt; and &lt;structural_error&gt; elements instead of the <process_error> tag.",
"&lt;stochastic_error&gt;",      "[OPTIONAL] irreducible uncertainty that is associated with natural stochastic processes (e.g., demographic stochasticity, disturbance)",
"&lt;structural_error&gt;",      "[OPTIONAL] uncertainty associated with human ignorance about the true process (e.g., model structure) and numerical approximations."
),
caption = "Table 6: Uncertainty classes",
label = "uncertainty"
)
```


Every element in Table 6 needs to be reported at least once, even if the metadata simply states that a specific term is absent from the model, or that the term is present but the forecast does not consider any uncertainty. Box 3 provides an example of the EML uncertainty elements for our Lotka-Volterra case study (see Introduction: A Simple Example), which is a simple dynamic model that predicts two state variables using six parameters, no random effects, no drivers/covariates, and both observation and process error. Each uncertainty class has the same basic structure for its component subelements (although some have some special cases described below). 
An uncertainty element (Table 6) can be repeated if different terms within the forecasting process have different subelements. For example, a model may have one subset of \<drivers\> that are data-driven and propagate uncertainty (e.g., weather forecast) and another subset that are scenario-based. Similarly, a process-based model may have one subset of \<parameters\> that are fixed constants, another subset that are calibrated a priori, and a third subset that are dynamically updated via data assimilation.

```
<initial_conditions>
          <present>TRUE</present>
          <data_driven>TRUE</data_driven>
          <complexity>2</complexity>
          <propagation>
                      <type>ensemble</type>
                      <size>10</size>
          </propagation>
</initial_conditions>
<drivers>
          <present>FALSE</present>
</drivers>
<parameters>
          <present>TRUE</present>
          <data_driven>TRUE</data_driven>
          <complexity>6</complexity>
</parameters>
<random_effects>
          <present>FALSE</present>
</random_effects>
<obs_error>
          <present>TRUE</present>
          <data_driven>TRUE</data_driven>
          <complexity>1</complexity>
          <covariance>FALSE</covariance>
<obs_error>
<process_error>
          <present>TRUE</present>
          <data_driven>TRUE</data_driven>
          <complexity>1</complexity>
          <covariance>FALSE</covariance>
          <propagation>
                      <type>ensemble</type>
                      <size>10</size>
          </propagation>
</process_error>
```
Figure 5: Example Extensible Markup Language (XML) for the uncertainty classes

### \<present\> subelement [REQUIRED]
Within each uncertainty class, the \<present\> subelement contains a boolean value (TRUE/FALSE) that is used to indicate whether the model contains this concept. For example, a model might have parameters (TRUE) but not random effects (FALSE). Similarly, a regression-style model would not have an initial condition because the predicted state, Y, does not depend on the current state. If a concept is absent from the model, the forecast cannot consider uncertainty associated with it and thus none of the other uncertainty elements below should be included. 

### \<data_driven\> subelement [REQUIRED if present = TRUE]
Similar to \<present\>, \<data_driven\> is a boolean (TRUE/FALSE) element used to indicate whether or not a specific input was derived from data (e.g., calibrated model parameters, a single time series of observed meteorological driver data). For sake of internal consistency, quantitative forecasts of other variables that are used as inputs into ecological forecasts (e.g., weather forecasts) should be treated as data but scenarios should not. Other examples of non-data-driven inputs include spin-up initial conditions and hand-tuned or theoretical parameters.

### \<complexity\> subelement [RECOMMENDED if present = TRUE]
Within each uncertainty class, the “complexity” subelement is a positive integer used to help classify the complexity of different modeling approaches in a simple, understandable way. Specifically, this element should list the size/dimension of each uncertainty class at a **single location**. For example, a forecast that takes in one initial condition for each of 500 grid cells would still have a complexity of 1.

* **initial_conditions**: number of state variables in the model. Examples of this would be the number of species in a community model, number of age/size classes in a population model, or number of pools in a biogeochemical model. 
* **drivers**: number of different driver variables or covariates in a model. For example, in a multiple regression this would be the number of X’s. For a climate-driven model, this would be the number of climate inputs (temperature, precipitation, solar radiation, etc.).
* **parameters**: number of estimated parameters/coefficients in a model at a single point in space/time. For example, in a regression it would be the number of slopes and intercepts. This number can be non-integer for methods that estimate an effective number of parameters (e.g., generalized additive models [GAMs], hierarchical models).
* **random_effects**: number of random effect terms, which should be equivalent to the number of random effect variances estimated. For example, if you had a hierarchical univariate regression with a random intercept you would have two parameters (slope and intercept) and one random effect (intercept). As of 2023, the convention does not record the number of distinct observation units that the model was calibrated from. So, in our random intercept regression example, if this model was fit at 50 sites to be able to estimate the random intercept variance, that would affect the uncertainty about the mean and variance but that ‘50’ would not be part of the complexity dimensions.
* **obs_error, process_error**: dimension of the error covariance matrix. For example, if we had a n x n covariance matrix, n is the value entered for \<complexity\>. Typically, n should match the dimensionality of the `initial_conditions` unless there are state variables where process error is not being estimated or propagated. Process and observation error are special cases that have additional recommended subelements:
    * \<covariance\>: TRUE = full covariance matrix, FALSE = diagonal only,
    * \<localization\>: Text. If covariance = TRUE, describe any localization approach used.

### \<propagation\> subelement
This uncertainty element is used to indicate that the model propagates uncertainty about this term into forecasts. A common example of this is a model run multiple times (i.e., ensemble) that samples the distributions of parameters, initial conditions, or drivers. Alternatively, one might be using an analytical approach to estimate how input uncertainties for a specific term translates into output uncertainties. The \<propagation\> element has several recommended subelements that are used to document the approaches used for uncertainty propagation. A specific value is not reported under \<propagation\> itself. If subelements are not included users should include an empty tag, \<propagation\>\</propagation\>, to indicate that uncertainty was propagated.

Subelements:

* \<type\> - “ensemble” or “analytic,”
* If type = ensemble
    * \<size\> = number of ensemble members,
* If type = analytic
    * \<method\>  text.

In terms of subelements, the \<type\> element distinguishes between analytical approaches to uncertainty propagation (e.g., quadrature, analytical moments, derivative / adjoint based methods) and numerical methods (ensembles, Monte Carlo simulation). For analytical approaches the convention requires a short text description of the \<method\>, while for numerical methods it requires the ensemble \<size\>.

### \<assimilation\> subtag
This element is used to indicate that a model iteratively updates this term through data assimilation. An example would be using a formal variational or ensemble (e.g., Ensemble Kalman Filter [EnKF], Particle Filter [PF]) data assimilation approach. For simpler models, this would also include iteratively refitting the whole model to the combination of the new and old data. Similar to \<propagation\>, this subelement does not have a single value, but documents the approaches used for data assimilation using the following recommended subelements:

* \<type\> - simple title for the approach used (e.g., PF, EnKF),
* \<reference\> - citation, DOI, or URL for the method used,
* \<complexity\> - directly analogous to the complexity subtag but describing the number of states, parameters, variances, etc that are iteratively updated. For spatially explicit forecasts, this would be the complexity at a single location or grid cell (e.g., a forecast that updates one state variable at 500 locations would still have a complexity of 1),
* \<attributeName\> - OPTIONAL element (one per variable) to list the variables being updated, which can be handy if only a subset of variables are updated. This element should match the attributeNames in the equivalent metadata “entity” (see below).

## Metadata validator and metadata helper functions

To assist users in adopting the EFI forecasting metadata convention we have developed an R-based metadata validation tool. This tool builds upon the base EML validation tool in the R EML package [@boettiger_eml_2022] but adds checks for the EFI-specific variables added in the additionalMetadata (see subsection “Forecast uncertainty”). Future planned directions are to extend the validator tool to other languages (e.g., Python) and to predefine customUnits within EML so that UDUNITS will validate.

In addition to the validation tool, the EFI Research Coordination Network has made a R package, neon4cast (https://github.com/eco4cast/neon4cast), that provides a suite of tools around its NEON Ecological Forecasting Challenge, which include a set of helper functions around metadata creation (`neon4cast::generate_metadata`) and output file validation (`neon4cast::forecast_output_validator`). The tools are somewhat customized to the five NEON challenge areas (aquatic ecosystems, terrestrial water and carbon fluxes, tick populations, plant phenology, beetle communities) but provide a useful template.