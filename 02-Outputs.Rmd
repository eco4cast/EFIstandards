# FORECAST OUTPUT DATA STRUCTURES

## Design Assumptions

In developing a convention for how to store ecological forecasts, three key features were considered central to any design. First, as noted earlier, not only are forecasts quantitative and specific, but they are also typically probabilistic and include a robust accounting of uncertainties. Thus, capturing forecast uncertainties is an essential feature of any output storage format. Furthermore, these uncertainties are often highly structured, with complex covariances across space, time, and state variables that are important to preserve. Such covariances are important to capture if one ever needs to aggregate (sum, integrate) forecasts over space or time, detect changes in space or time, or calculate differences, as approaches that fail to account for these covariances can be massively misleading (NASA Carbon Monitoring System Uncertainty Working Group, written commun. 2022). Second, ecological forecasts frequently use Monte Carlo methods to propagate uncertainties (i.e., using ensembles) so it was important to be able to store individual ensemble members. Preserving ensembles greatly facilitates the correct handling of covariances. Third, ecological forecast outputs are frequently high-dimensional (e.g., ensembles of multiple state variables through time and across multiple spatial locations) so it was important that data be easy to organize, access, and process, by dimension.

In the sections below we first define the EFI forecast output convention in the abstract, and then illustrate the application of this convention to the two file formats that EFI has currently adopted: netCDF (Network Common Data Form) and CSV. (comma-separated values). netCDF has the advantage of being self-documenting, more compact, and more flexible when working with high-dimensional data (especially when not all variables have the same dimensions). CSV, on the other hand, is more familiar to a broader audience, especially among non-academic end users, but is more reliant on external metadata. That said, the convention is defined such that the combination of output and metadata files allows the two file formats to be interconverted with no loss of information. More broadly, the EFI convention is defined in general enough terms that it is applicable to new and emerging file formats (e.g., parquet, zarr). Indeed ,netCDF has recently extended its data model to support the zarr file format.

The EFI forecast output convention consists of four components, each described in a subsection below: (1) global attributes used to track the provenance of the forecast, (2) the dimensions of the forecast (e.g., time, space, uncertainty), (3) the output variables being forecast, and (4) ancillary indicator variables that aid in interpreting output variables.

## Global attributes

For “global attributes” the EFI convention provides up to four unique identifiers for any forecast: a target_id identifying what the forecast is scored against; a model_name that can link across multiple model versions; a model_version that connects all forecasts produced by a specific version of a forecast model and workflow; and an iteration_id for that specific forecast (Table 1). These elements are part of the EML metadata and the output file’s internal metadata for the netCDF format, and are recommended as additional outer columns in the CSV format, especially when forecasts are expected to be used in multimodel predictions or syntheses. The hierarchical order of these variables reflects their potential use as additional outer dimensions in such syntheses (i.e., a given target_id can be predicted by multiple models, a single model_name can have multiple model_versions, a single model_version can be used make many forecasts with unique iteration_ids).

```{r,echo=FALSE}
knitr::kable(
  tibble::tribble(
    ~Attribute,      ~Description,
    "target_id",     "(OPTIONAL) Unique identifier pointing to data or metadata about what the forecast is being scored against",
    "model_name",    "Unique identifier for a forecasting project that can be used to link across different model versions",
    "model_version", "(RECOMMENDED) Unique identifier for a specific forecast model/workflow version",
    "iteration_id",       "(OPTIONAL) Unique identifier for a specific forecast run. Important to include in cases where a forecast might be rerun (e.g., real-time forecast versus reanalysis)"
  ),
caption = "Table 1: Global attributes (metadata) for netCDF forecast files. See Figure 3 for an example application.",
label = "GlobalAttributes"
)
```


First, target_id, which is optional, is a unique identifier (e.g., Uniform Resource Location [URL], Digital Object Identifier [DOI]) that links to data or metadata about what the forecast is being scored against. The idea of the target_id is to facilitate intercomparison by being able to definitively say that two (or more) different forecasts were trying to predict the same thing (e.g., in a forecasting challenge). For example, one of the NEON Ecological Forecasting Challenges was to predict the Green Chromatic Coordinate (GCC) observed by phenological cameras at Harvard Forest, Massachusetts; in this case, all of the forecasts would have the same target_id corresponding to a URL to this dataset. As of January 2023 the EFI standard does not specify requirements about what the target_id can validly point to (e.g., raw data versus standardized machine-readable metadata describing a forecast’s ‘rules’), but this is an area of active development.

The model_name is a unique identifier that links across different model versions. Examples might include the name or acronym for a pre-existing process-based model, a project code repository, URL, or a forecasting competition team name.

The model_version is a unique identifier for a specific version of a forecast model and workflow. This identifier should update when the model is updated or when the underlying forecast workflow is updated (e.g., model recalibration, switching sources for driver/covariate data, adding additional data constraints, changes to observation operators). Specifically, results from a single model_version can be considered as coming from the same system and thus are comparable. That said, algorithms that learn iteratively over time (e.g., reinforcement learning or including model parameters within iterative data assimilation) only require a new model_version when the underlying algorithm is updated, not for every incremental update of the learning process itself. EFI recommends issuing DOIs for different model/workflow versions, and thus this is a natural choice for a model_version. 

The iteration_id is a unique identifier for a specific forecast run (character string). The datetime for the start of the forecast is generally most convenient, but it could be any alternative system-specific identifier (e.g., database ID, content identifier) (Farrell et al. 2013, Boettiger and Poelen 2021). That said, EFI recommends against issuing a DOI for an individual forecast, as will be discussed below in “Forecast Archiving”. In brief, DOIs are typically associated with persistent, unchanging archives. For iterative forecasts, there are many reasons to archive batches of forecasts over a specific period (e.g., one year) rather than to mint a new DOI every time a new forecast is issued (e.g., daily) or to use a single DOI to reference a forecast record that is being updated iteratively.

If users need to store forecasts that come from different model_names, model_versions and iteration_ids in the same file (e.g., for multi-model ensembles or forecast inter-comparisons) then the set of attributes needed to identify a forecast within the file should be added as additional dimensions ahead of the time dimension and should be entered in the order indicated in Table 1 (i.e., with iteration_id as the innermost dimension that comes right before time (see below)).

## Dimensions

A core part of the EFI convention is the definition of variable dimensions (Table 2). Building upon the Climate and Forecast (CF, http://cfconventions.org/) (Eaton et al. 2020) and Cooperative Ocean/Atmosphere Research Data Service (COARDS 1995) conventions, the order of dimensions for all file formats is T, Z, Y, X, U where T is time, Z, Y, and X are spatial dimensions, and U represents forecast uncertainty (e.g., ensemble member or summary statistic). Each row in the file thus represents a unique datetime, location, etc. That said, for any particular application, not all dimensions may be required. For example, Tables 3 and 4 shows the top few rows of the Lotka-Volterra example forecast (see Introduction: A Simple Example) written out in the ensemble CSV format (Table 3) and probability distribution CSV format (Table 4) respectively. Because this forecast is for a single location only time, depth, and uncertainty are required (X and Y would be recorded in the metadata).

```{r,echo=FALSE}
knitr::kable(
tibble::tribble(
~Dimension,           ~Description,
"reference_datetime", "ISO 8601 (ISO 2019) datetime the forecast starts from (a.k.a. issue time); Only needed if more than one reference_datetime is stored in a single file. Forecast lead time is thus datetime - reference_datetime. In a hindcast the reference_datetime will be earlier than the time the hindcast was actually produced (see pubDate in Section 3). Datetimes are allowed to be earlier than the reference_datetime if a reanalysis/reforecast is run before the start of the forecast period. This variable was called start_time before v0.5 of the EFI standard.", 
"datetime",           "ISO 8601 (ISO 2019) datetime being predicted; follows CF convention http://cfconventions.org/cf-conventions/cf-conventions.html#time-coordinate. This variable was called time before v0.5 of the EFI convention. <br /><br /> For time-integrated variables (e.g., cumulative net primary productivity), one should specify the start_datetime and end_datetime as two variables, instead of the single datetime. If this is not provided the datetime is assumed to be the MIDPOINT of the integration period.", 
"depth or height",    "No single standard name for the Z dimension. Where possible, CF conventions for vertical dimension names and attributes (https://cfconventions.org/cf-conventions/cf-conventions.html#vertical-coordinate) should be used.",
"lon or X",           "Longitude (units = 'degrees_east') is the default spatial coordinate. The alternative use of Y, X for spatial coordinates should conform to the CF convention and requires additional metadata about grids and projections.",
"lat or Y",           "Latitude (degrees_north)",
"site_id",            "For forecasts that are not on a spatial grid, use of a site dimension that maps to a more detailed geometry (points, polygons, etc.) is allowable. In general this would be documented in the external metadata (e.g., a look-up table that provides lon and lat); however in netCDF this could be handled by the CF Discrete Sampling Geometry data model.",
"family",             "For ensembles: 'ensemble.' Default value if unspecified. <br /><br /> For probability distributions: Name of the statistical distribution associated with the reported statistics. The 'sample' distribution is synonymous with 'ensemble.' <br /><br /> For summary statistics: 'summary.' <br /><br /> If this dimension does not vary, it is permissible to specify family as a variable attribute if the file format being used supports this (e.g., netCDF).",
"parameter",           "REQUIRED <br /><br /> For ensembles: Integers 1 to Ne (Ne = total size of ensemble) Note: for backward compatibility this can alternatively be named “ensemble” but this is planned to be deprecated in future versions. <br /><br /> For named distributions: parameter/statistic being specified (e.g., mean, standard deviation)",
"obs_flag",            "Flag indicating whether observation error has been included in the prediction. Only REQUIRED if forecasting both the latent and observed state."
),
caption = "Table 2: Ecological forecast dimensions in the order that should be used to specify variables (time, space, uncertainty). The only required dimension is parameter; other dimensions can be dropped if they only have a single value and that value is clearly documented in the metadata. Global attributes (Table 1) can also optionally be used as outer dimensions if needed.",
label = "dimensions"
)
```

```{r,echo=FALSE}
knitr::kable(
  tibble::tribble(
~"reference_datetime &lt;date&gt;", ~"datetime &lt;date&gt;",~"depth &lt;dbl&gt;",~"family &lt;chr&gt;", ~"parameter &lt;int&gt;",~"obs_flag &lt;int&gt;", ~"variable &lt;chr&gt;",~"prediction &lt;dbl&gt;",
'2001-03-04', '2001-03-05', 1.0, "sample", 1, 1, "species_1", 0.983,
'2001-03-04', '2001-03-05', 1.0, 'sample', 1, 1, 'species_2', 1.946,
'2001-03-04', '2001-03-05', 3.0, 'sample', 1, 1, 'species_1', 0.972,
'2001-03-04', '2001-03-05', 3.0, 'sample', 1, 1, 'species_2', 1.948,
'2001-03-04', '2001-03-05', 5.0, 'sample', 1, 1, 'species_1', 0.985,
'2001-03-04', '2001-03-05', 5.0, 'sample', 1, 1, 'species_2', 1.954,
'2001-03-04', '2001-03-05', 1.0, 'sample', 2, 1, 'species_1', 0.974,
'2001-03-04', '2001-03-05', 1.0, 'sample', 2, 1, 'species_2', 1.950,
'2001-03-04', '2001-03-05', 3.0, 'sample', 2, 1, 'species_1', 0.956,
'2001-03-04', '2001-03-05', 3.0, 'sample', 2, 1, 'species_2', 1.956,
'2001-03-04', '2001-03-05', 5.0, 'sample', 2, 1, 'species_1', 0.958,
'2001-03-04', '2001-03-05', 5.0, "sample", 2, 1, "species_2", 1.957
  ),
caption = "Table 3: Ensemble CSV format for Lotka-Volterra example (Section 1.1), where parameter designates ensemble number. Only 12 of 3600 rows are shown.",
label = "EnsembleExample"
)
```


```{r,echo=FALSE}
knitr::kable(
  tibble::tribble(
~"reference_datetime &lt;date&gt;", ~"datetime &lt;date&gt;", ~"depth &lt;dbl&gt;", ~"family &lt;chr&gt;", ~"parameter &lt;chr&gt;", ~"obs_flag &lt;int&gt;", ~"variable &lt;chr&gt;", ~"prediction &lt;dbl&gt;",
2001-03-04, 2001-03-04, 1.0, 'normal', 'mu',    1, 'species_1', 0.756,
2001-03-04, 2001-03-04, 1.0, 'normal', 'sigma', 1, 'species_1', 0.174,
2001-03-04, 2001-03-04, 1.0, 'normal', 'mu',    1, 'species_2', 0.250,
2001-03-04, 2001-03-04, 1.0, 'normal', 'sigma', 1, 'species_2', 0.013,
2001-03-04, 2001-03-04, 1.0, 'normal', 'mu',    2, 'species_1', 0.756,
2001-03-04, 2001-03-04, 1.0, 'normal', 'sigma', 2, 'species_1', 0.174,
2001-03-04, 2001-03-04, 1.0, 'normal', 'mu',    2, 'species_2', 0.250,
2001-03-04, 2001-03-04, 1.0, 'normal', 'sigma', 2, 'species_2', 0.013,
2001-03-04, 2001-03-04, 3.0, 'normal', 'mu',    1, 'species_1', 0.982,
2001-03-04, 2001-03-04, 3.0, 'normal', 'sigma', 1, 'species_1', 0.347
  ),
caption = "Table 4: Lotka-Volterra example forecast (Section 1.1) written in distributional CSV format with a Normal distribution family. The “summary” format, which does not imply a distributional assumption, would be analogous to this but with family = “summary” and parameters “mean” and “sd” (See Table S2). Only 10 of 720 rows shown.",
label = "DistributionalExample"
)
```

### Time: 
In the EFI convention, datetimes are specified in International Organization for Standardization (ISO) 8601 format, YYYY-MM-DDThh:mm:ssZ (ISO 2019). The T is the ISO standard delimiter between date and time. The trailing Z indicates that Coordinated Universal Time (UTC) is the default time zone, but alternate time zones can be specified as offsets after the time (e.g., -05:00 for Eastern Standard) in place of the Z (i.e., Z indicates zero offset). Within ISO 8601, date and time terms can be omitted from right to left to express reduced accuracy; for example, May 2020 would just be 2020-05. Note also, that within netCDF files the convention is to express the time dimension relative to a user-specified origin (e.g., days since 2020-01-01), in which case the origin should be in ISO standard and the time increments since the origin are in UDUNITS (see 2.4 Forecasted Variables below). The ISO standard also allows the specification of weeks and day-of-week as an alternative to months and day-of-month by using the W prefix (e.g., 2022-W02-03 specifies the third day of the second week of the year). ISO weeks start on Mondays and week 01 is the week with the first Thursday of the year in it.

Unlike typical time-series data, forecasts have two time dimensions – the reference_datetime from which a forecast starts and the datetime being predicted. In particular, iterative forecasts will frequently make many predictions for a specific datetime that were issued at different lead times. To clarify, reference_datetime is essentially the t=0 in the forecast model, and the horizon of a forecast is the difference between datetime and reference_datetime. For a “true” forecast, the forecast publication time (a.k.a. issue time, see pubDate in “Forecast Dataset Metadata”) should be close to the reference_datetime, with the difference being the latency associated with running and posting the forecast. For a hindcast or reforecast, the reference_datetime can be much earlier than the pubDate. In practice, forecasts issued at different dates or times are usually stored in separate files, and thus the datetime dimension is the time being predicted. If multiple forecasts are placed within a single file, then the reference_datatime is the first time dimension and then the datetime being predicted is the second. Furthermore, for time-integrated variables (i.e., variables that represent a mean or cumulative over some time period rather than an instantaneous observation) the datetime dimension should explicitly be split into a start_datetime and end_datetime rather than relying on potentially ambiguous (and less machine parsable) implicit definitions within variable descriptions. Finally, the specific names reference_datetime, datetime, start_datetime, and end_datetime were selected to be interoperable with the SpatioTemporal Asset Catalogue [STAC] forecasting extension (https://github.com/stac-extensions/forecast).

### Space:
The spatial dimensions are developed with the default assumption that the spatial domain is regular (e.g., on a grid). Following CF convention, the X,Y coordinate is given in longitude and latitude using lon and lat as standard names and UDUNITS compliant units (e.g., decimal degrees). Other spatial projections are also possible, but should conform to the CF convention (https://cfconventions.org/cf-conventions/cf-conventions.html#grid-mappings-and-projections). If spatial dimensions are lat-lon, the convention assumes EPSG:4326. If spatial dimensions are given as X-Y, a CF-compliant coordinate grid specification is required. For other geometries (e.g., non-contiguous points, vector polygons) a site_id dimension is used to map identifiers to a set of attributes or look-up table with more detailed geometry information (the CF convention refers to this as a Discrete Sampling Geometry, https://cfconventions.org/Data/cf-conventions/cf-conventions-1.10/cf-conventions.html#discrete-sampling-geometries). For example, if one were to use netCDF to store forecasts of leaf area index (LAI) across NEON sites, LAI might have dimensions LAI[datetime,site_id] while there would also be variables lon[site_id] and lat[site_id] storing the location of the NEON sites. Similarly, using additional dimensions to indicate nested hierarchical designs (e.g., plots within sites) is recommended (but not required), but users should document these dimensions in the metadata and order dimensions from coarsest to finest (e.g., LAI[datetime,site_id, plot_id, subplot_id]).
The vertical dimension should be indicated as height or depth. Units of height should be documented in the metadata and should be UDUNITS compliant, with meters being the preferred international system of units (SI) standard (https://cfconventions.org/Data/cf-conventions/cf-conventions-1.10/cf-conventions.html#vertical-coordinate). Per CF convention, metadata should document the attribute of whether the positive direction is up or down. If any of the spatial dimensions require the specification of a datum, projection, or reference height, this should be documented in the metadata. Finally, spatial dimensions are optional in the output file if they only include one value (e.g., forecasts at a single site or forecasts where predictions do not change with height/depth) because this information is required in the metadata. 

### Uncertainty:
The uncertainty dimension is a key focus and key feature of the EFI convention, which is designed around archiving probabilistic forecasts. The most common case for this is the prediction of a continuous response variable (e.g., biomass) where the probability is represented using a probability density function (pdf). Although we earlier presented U as a single dimension, in practice information about this uncertainty is encoded through three variables: family, parameter, and obs_flag, although in many cases only parameter is required. To understand what these variables mean and how to use them, consider two alternative ways of representing uncertainty: (a) using parameters to describe a probability distribution, e.g., N(𝜇,𝜎2), or (b) using random samples from these predictive distributions (a.k.a. ensemble members), such as when using Monte Carlo methods (e.g., Markov Chain Monte Carlo [MCMC], sequential Monte Carlo [SMC], bootstrapping). A specific example of this is the Lotka-Volterra case study (see Introduction: A Simple Example), which provided stochastic, ensemble-based predictions of two species at three depths that accounted for uncertainty in initial conditions and process error. Examples of how to apply the EFI convention to store this case study in netCDF and CSV formats are provided in the subsection “File Formats” following this conceptual explanation of the convention.

If Monte Carlo methods are used to make a forecast, then preserving the ensemble members themselves (option b) is strongly preferred over distributional parameters (option a) because just saving summary statistics results in a loss of information (e.g., shapes of distributions). This is particularly true for handling the covariances across state variables, locations, and times, which are often substantial. When working with ensembles, the family variable should be set to “ensemble,” in which case the parameter dimension is just an indexing variable for the ensemble members (e.g., 1…Ne). For example, in our Lotka-Volterra case study Ne = 10, so when written out in netCDF the forecast for each species would have a parameter dimension of length 10, while in CSV a parameter column would specify, for each row in the output, which ensemble member it belonged to. When working with very large ensembles (e.g., MCMC output) thinning output is acceptable to keep file sizes manageable, though care should be taken to maintain an adequate effective sample size (e.g., Ne=5000, depending on the specific forecast problem). To maintain compatibility with CF, and backwards compatibility with earlier versions of the EFI draft standard, it is currently acceptable to use ensemble as a synonym for parameter when using ensemble-based approaches. Likewise, “ensemble” is the default family, meaning that a forecast that is only using ensemble-based methods has the option of dropping the family dimension.

If one is making probabilistic forecasts where the output is explicitly or implicitly a named probability distribution (option a), then the family variable should be set to the name of that distribution. Within the EFI convention we adopt the distributional naming convention for probability distributions adopted by the fable project (https://fable.tidyverts.org/) (O’Hara-Wild et al. 2021). Likewise, the column name family was adopted to increase interoperability with fable. For a given choice of distributional family, the parameters dimension is used to encode specific parameter values for that distribution, such as the normal mean (mu) and standard deviation (sigma). For example, if we had analytically propagated uncertainty in our Lotka-Volterra case study using a Normal distribution then the netCDF forecast for each species would have a family dimension of length 1 to specify the distribution assumed (normal in this case) and a parameter dimension storing that distributions parameters (e.g., length 2 for mu and sigma). In CSV the same forecast would have both family and parameter columns and would require two rows to specify each prediction (e.g., one specifying normal, mu and the other specifying normal, sigma). To enter the covariance between two variables (e.g., in the multivariate_normal) enter cov as the parameter and use a hyphen as the delimiter between the two variable names. It is worth noting that parameter is the only required dimension in the EFI convention. For other dimensions it is acceptable to drop a dimension if it only has a single value that is documented in the metadata (e.g., single location, single time, default “ensemble” family). Supplementary Table S1 lists the current distributional families and parameters.

Probabilistic forecasting approaches that do not involve either ensembles or probability distributions can use the “summary” family and the values in Table S2 as parameters. Forecasts that produce a single realization (e.g., a predicted probability of occurrence, or a model run without any uncertainty propagation) have two alternatives. The preferred option is to set the ensemble size to 1. The other option is to use a distribution that produces a point estimate (e.g., Normal with standard deviation of 0) or the summary family with just a mean. In either case retaining the parameter dimension is important to ensure consistent processing of files by end users and standardized tools.

The final uncertainty dimension is obs_flag, the observation error flag, which is an indicator variable that records whether observation error had been included in the forecast. The default is to assume that the observation error is present (i.e., the ensemble quantiles would produce a predictive interval). If all forecast variables include observation error, then this flag is optional. By contrast, this flag is REQUIRED if a file includes a mix of confidence and predictive intervals (i.e., latent and observable variables) as otherwise the same variable name would exists in both confidence and predictive interval forms. Indeed, if the file format allows it (e.g., netCDF),  variables in a file can vary in whether they have an obs_flag dimension or not. Furthermore, when required, the first slot should store the latent state (confidence interval) because models that produce latent states tend to be able to do so for all variables, while observation error may only need to be added to a subset of variables for comparison to data. Because a model could theoretically be compared to multiple sensors that ostensibly measure the same thing, but with different error characteristics, an obs_flag dimension can have a length >2. If this is the case, the file metadata should clearly describe the different observation error cases.

## Forecasted Variables
The third part of the EFI output convention concerns the names and units of the output variables being forecasted. We use the Climate and Forecast (CF) convention for constructing variable names and units (Eaton et al. 2020). CF names should be composed of letters, digits, and underscores and it is recommended that names not be distinguished by uppercase or lowercase (i.e., if case is dropped, names should not be the same). CF names are typically written in lowercase with underscore separating words (e.g., net_primary_productivity). Note also that hyphens are prohibited within variable names because the convention uses hyphens as the delimiter when specifying covariances.

Any variable units within the data file should be SI and formatted to be machine-parsable by the UDUNITS library (https://www.unidata.ucar.edu/software/udunits/) (e.g., kg m-2). On a practical basis, we recommend using functions such as R’s units::ud_are_convertible to verify units are correctly formatted (Pebesma et al. 2022). 

As will be described in the “File Formats” subsection, the formatting of the output data itself is handled slightly differently between the netCDF and CSV formats. netCDF allows each variable to be its own object within the file, whereas in CSV output variables are stored in a long format, with column names for variable, and prediction coming immediately after the previously discussed dimension columns.

## Ancillary Indicator Variables

In addition to the forecasted variables, the EFI convention also defines four other standard variables: a required forecast flag, a recommended data_assimilation flag, an optional data assimilation quality control flag (da_qc), and an optional ensemble log_weight (Table 5).

```{r,echo=FALSE}
knitr::kable(
  tibble::tribble(
~Variable,           ~Description,
"data_assimilation", "[RECOMMENDED] Did data assimilation occur (1) or not (0) at that
time step, location, etc.",
"da_qc",             "[OPTIONAL] Was the data assimilation successful (0) or not (1 or error code)",
"forecast",          "[OPTIONAL] Was this timestep a forecast (1) or a hindcast (0)",
"log_weight",        "[OPTIONAL] Weight assigned to each ensemble member, natural log scale"
  ),
caption = "Table 5: Additional ecological forecast netCDF variables (beyond the forecast variables themselves).",
label = "AncillaryVars"
)
```

Similar to the forecast flag, data_assimilation is a boolean flag that records whether (1) or not (0) observational data were used to constrain the system state or parameters at that point in time. If the same time point exists twice, once without data assimilation (data_assimilation = 0) and the other with data_assimilation = 1, the former is assumed to be the Forecast step, and the latter is assumed to be the Analysis step within the Forecast-Analysis cycle (Dietze 2017a). Closely related to this is the optional data assimilation quality flag, da_qc, which records quality control information about a given assimilation step: 0 is used to encode success; 1 is used to indicate a general error; and positive integers greater than 1 are used to indicate system-specific failures documented in the metadata. Like the forecast flag, data_assimilation and da_qc will typically have a time dimension.

The final variable, log_weight, is used to record any weights assigned to each ensemble member. This optional variable is primarily used in data assimilation algorithms that iteratively weight the different ensemble members (e.g., particle filters). Weights are stored on a natural log (ln) scale to reduce numerical round-off issues. To allow for greater flexibility in algorithms, a sum-to-one constraint is not required (e.g., users may choose to record underlying scores, such as logLikelihoods). Because of this end users should note that sum-to-one normalization will need to be applied to perform analyses with weights. Those storing raw scores as their weights are strongly encouraged to document the meaning of such scores in their metadata.

## File Formats

### netCDF:
netCDF is a set of self-documenting, machine-independent data formats. It is particularly well suited for storing large and higher-dimensional data and for situations when different parts of a data set have different dimensions (e.g., mix of vectors, matrices, and high-dimensional arrays). Although less familiar to many ecologists, netCDF is commonly used in the physical environmental sciences (e.g., ObsPack format for greenhouse gas measurements (Masarie et al. 2014)) and by the ecological modeling community. This format has a long history (started in 1998), is well supported by common programming languages (e.g., R, Python), and tools for archiving, manipulating, and visualizing netCDF are well established (e.g., CDO, ncview, panoply, THREDDS/OpenDAP). For these reasons netCDF was selected as the preferred file format for archiving ecological forecasts.

A netCDF file consists of three parts (Hassell et al. 2017): dimensions, which describe the size of variables (e.g., 5 depths, 20 time points); variables, which store data of different dimensions; and attributes providing additional arbitrary metadata corresponding to either the entire file (see subsection “Global Attributes”) or specific variables (variable attributes; e.g., description, units, sign conventions, fill values for invalid/missing data) (Box 1). 

Most of the variables in a netCDF file should be the forecasted systems states, pools, and fluxes. Unlike the CSV format, where all the data are in one large table, netCDF files store each forecasted quantity in a dedicated variable, and different variables can have different labelled dimensions (“coordinates”) (Box 1). For example, one might forecast net_primary_productivity with dimensions [datetime, lon, lat, parameter], and in the same file have a forecast of mass_content_of_water_in_soil_layer with dimensions [datetime, depth, lon, lat, parameter]. In each of these cases, the dimension corresponds to the integer size of a particular axis and is paired with a dedicated 1-dimensional coordinate variable of the same size that provides the labels along that dimension. In the net_primary_productivity above, if the forecast is hourly over 3 days, then the datetime dimension has an integer value of 24 × 3 = 72 and is accompanied by a dedicated variable called datetime that is a 1-dimensional vector of length 72 containing the actual timesteps. As noted earlier, dimensions should follow the EFI convention names and order. If one is using a site dimension for the variables (e.g., if forecast locations are for a collection of points that are not on a grid), then following the NetCDF Discrete Sampling Geometry data model, the spatial locations of the sites should be defined as additional 1-dimensional vectors with corresponding site dimensions (e.g., lat[site], lon[site]). 

```{fig.cap="Figure 3: netCDF header for our example forecast (Section 1.1), illustrating how dimensions, variables, and attributes are structured."}
netcdf logistic-forecast-ensemble-multi-variable-space-long {
dimensions:
	datetime = 30 ;
	depth = 3 ;
	parameter = 10 ;
	obs_flag = 2 ;
variables:
	double datetime(datetime) ;
		datetime:units = "days since 2001-03-04" ;
		datetime:long_name = "datetime" ;
	double depth(depth) ;
		depth:units = "meters" ;
		depth:long_name = "Depth from surface" ;
	int parameter(parameter) ;
		parameter:long_name = "ensemble member" ;
	int obs_flag(obs_flag) ;
		obs_flag:long_name = "observation error flag" ;
	float species_1(datetime, depth, parameter, obs_flag) ;
		species_1:units = "number of individuals" ;
		species_1:long_name = "<scientific name of species 1>" ;
	float species_2(datetime, depth, parameter, obs_flag) ;
		species_2:units = "number of individuals" ;
		species_2:long_name = "<scientific name of species 2>" ;
	float data_assimilation(datetime) ;
		data_assimilation:units = "integer" ;
		data_assimilation:long_name = "EFI standard data assimilation code" ;
// global attributes:
		:model_name = "LogisticDemo" ;
		:model_version = "v0.5" ;
		:iteration_id = "20010304T060000" ;
}
```
Figure 3: netCDF header for our example forecast (Section 1.1), illustrating how dimensions, variables, and attributes are structured.

### CSV:
The CSV format is less efficient than netCDF (in terms of file size, data access performance, and flexibility of data extraction/manipulation) and is much more reliant on external metadata for information like variable name explanations and units. That said, provided the same numerical precision is used and metadata provided (see section “Forecast Dataset Metadata”), CSV can preserve the same information content as the netCDF. We anticipate the CSV format to be most useful: (1) for simple, low-dimensional forecasts; (2) when forecast producers are unaccustomed to netCDF; or (3) as a conversion format from netCDF when forecast user communities are unaccustomed to netCDF. 

Unless otherwise noted, the CSV format begins with the dimensions in the standard order and naming (Table 2). Forecast outputs are then stored in a long format using the standard column names variable and prediction. The variable column will typically be character based, storing the CF-compliant variable names. The prediction column stores the numeric predictions for each variable, with the specific meaning dependent on how the family and parameter columns were specified (e.g., consecutive rows might be individual ensemble members or the parameters describing a specific probability distributions). The ancillary indicator variables (forecast, data_assimilation, da_qc, log_weight; Table 5) will be entered as additional columns after variable and prediction. This long format has the advantages of being easy to filter, sort, summarize, and append new rows onto, and is relatively compact if a lot of data are missing. The examples below illustrate how to write out our Lotka-Volterra case study (see Introduction: A Simple Example) in CSV format. The first example (Table 3) assumes an ensemble-based forecast with dimensions of datetime, depth, parameter, and obs_flag and the additional variables of forecast and data_assimilation. This file contains the same information with the same dimensions as the earlier netCDF example (Box 1). The second example (Table 4) is the same forecast done using a distribution-based parameterization, assuming a Normal error distribution. 
